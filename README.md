# Incubyte ETL Pipeline

This project is an ETL (Extract, Transform, Load) pipeline designed to load customer data from a source file into a PostgreSQL database and handle incremental updates based on specific conditions. The pipeline is designed to partition data by country and load customer records incrementally based on the consultation date.

## Table of Contents

- [Project Structure](#project-structure)
- [Requirements](#requirements)
- [Getting Started](#getting-started)
- [Pipeline Components](#pipeline-components)
  - [Extract](#extract)
  - [Load to Staging](#load-to-staging)
  - [Incremental Load](#incremental-load)
- [Logging](#logging)
- [How to Run](#how-to-run)
- [License](#license)

## Project Structure

data_pipeline/
├── incubyte/
│   ├── Include/
│   ├── Lib/
│   ├── Scripts/
│   └── pyvenv.cfg
├── logs/
│   └── etl_pipeline.log
├── source/
│   ├── dummy_customer_data.xlsx
│   └── dummy_data.txt
├── Tables/
│   ├── DDL/
│   │   ├── Curate_customer_info.sql
│   │   └── STG_Customer_Info.sql
│   └── Models/
├── trans/
│   └── data.csv
├── etl_pipeline.py
├── requirements.txt
├── test_etl_pipeline.py
└── util.py



- `incubyte/`: Contains virtual environment files.
- `logs/`: Stores log files generated by the pipeline.
- `source/`: Contains source files like the customer data file.
- `Tables/DDL/`: SQL files for creating staging and curated tables in the database.
- `trans/`: Stores transformed data before loading into the database.
- `etl_pipeline.py`: Main ETL pipeline script.
- `requirements.txt`: Python dependencies.
- `test_etl_pipeline.py`: Script for testing the ETL pipeline.
- `util.py`: Utility functions for logging and partition management.

## Requirements

- Python 3.8 or higher
- PostgreSQL database
- Required Python packages (specified in `requirements.txt`)

Install the requirements using:

```bash
pip install -r requirements.txt

## Pipeline Components

### Extract
The `extract` method reads a pipe-delimited text file where headers are marked with `H|` and data rows are marked with `D|`. The data is converted into a pandas DataFrame.

### Load to Staging
The `load_stg` method:
- Loads extracted data into a staging table in PostgreSQL.
- Ensures country-specific partitions are created in the staging and curated tables.
- Uses PostgreSQL's `COPY` command for efficient data loading.

### Incremental Load
The `incremental_load` method:
- Performs incremental loads based on the latest consultation date.
- Adds new records and updates existing ones in the curated table.
- Calculates age and days since last consultation, and filters records with more than 30 days since the last consultation.

## Logging
The pipeline logs important information and errors to `logs/etl_pipeline.log`.

## How to Run
1. Configure the logging path and database URI in `etl_pipeline.py`.
2. Run the ETL process:

   ```bash
   python etl_pipeline.py


